{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48021cf0",
   "metadata": {},
   "source": [
    "# RandomWalker Length 10 with one merge sanity checks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef2de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from util import TokenMergeBuffer, TokenUnmergeBuffer, EarlyStopper\n",
    "from models import AE\n",
    "from toy_data.environments import RandomWalker\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e850ecf",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133de16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "walker = RandomWalker(\n",
    "    n=1000, \n",
    "    length=10, \n",
    "    dim=2, \n",
    "    step_scale=0.1, \n",
    "    init=\"normal\", \n",
    "    drift=\"centre\").generate()\n",
    "\n",
    "dataset = torch.tensor(walker.data, dtype=torch.float32)\n",
    "\n",
    "train_pct = 0.8\n",
    "train_size = int(train_pct * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f63883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b076b6",
   "metadata": {},
   "source": [
    "## Train Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "698265f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merge_depth(curr_epoch, total_epochs, max_depth):\n",
    "    \"\"\"\n",
    "    For curriculum depth.\n",
    "    \"\"\"\n",
    "    progress = curr_epoch / total_epochs\n",
    "    depth = int(progress * max_depth)\n",
    "    depth = depth if depth > 0 else 1\n",
    "\n",
    "    return depth\n",
    "\n",
    "\n",
    "def visualise(orig, recon, show=False):\n",
    "\n",
    "    if isinstance(orig, torch.Tensor):\n",
    "        orig = orig.detach().cpu().numpy()\n",
    "    if isinstance(recon, torch.Tensor):\n",
    "        recon = recon.detach().cpu().numpy()\n",
    "\n",
    "    def plot_trajectory(traj, ax, color=\"blue\", label=None, linewidth=1, linestyle=\"solid\"):\n",
    "        for (x0, y0), (x1, y1) in zip(traj[:-1], traj[1:]):\n",
    "            ax.plot([x0, x1], [y0, y1], color=color, linewidth=1, linestyle=linestyle)\n",
    "\n",
    "        if label:\n",
    "            ax.plot([], [], color=color, label=label)\n",
    "\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "        return ax\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    plot_trajectory(orig, ax=ax, color=\"blue\", label=\"Original\", linewidth=2)\n",
    "    plot_trajectory(recon, ax=ax, color=\"red\", label=\"Reconstruction\", linestyle=\"solid\")\n",
    "\n",
    "    # ax.set_title(\"\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.legend()\n",
    "\n",
    "    # ax.set_xlim(-2, 2)\n",
    "    # ax.set_ylim(-2, 2)\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "    if show: \n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "        return fig\n",
    "    \n",
    "def visualise_multiple(orig_list, recon_list, show=False):\n",
    "    \"\"\"\n",
    "    orig_list, recon_list: lists of trajectories (numpy arrays or torch tensors)\n",
    "    \"\"\"\n",
    "    n = len(orig_list)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(6 * n, 6))\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, (orig, recon) in enumerate(zip(orig_list, recon_list)):\n",
    "        if isinstance(orig, torch.Tensor):\n",
    "            orig = orig.detach().cpu().numpy()\n",
    "        if isinstance(recon, torch.Tensor):\n",
    "            recon = recon.detach().cpu().numpy()\n",
    "\n",
    "        ax = axes[i]\n",
    "\n",
    "        # plot original\n",
    "        line_orig, = ax.plot([], [], color=\"blue\", linewidth=1)\n",
    "        for (x0, y0), (x1, y1) in zip(orig[:-1], orig[1:]):\n",
    "            ax.plot([x0, x1], [y0, y1], color=\"blue\", linewidth=1)\n",
    "        # plot reconstruction\n",
    "        line_recon, = ax.plot([], [], color=\"red\", linewidth=1)\n",
    "        for (x0, y0), (x1, y1) in zip(recon[:-1], recon[1:]):\n",
    "            ax.plot([x0, x1], [y0, y1], color=\"red\", linewidth=1)\n",
    "\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_title(f\"Sample {i+1}\")\n",
    "\n",
    "        if i == 0:\n",
    "            legend_lines = [line_orig, line_recon]\n",
    "\n",
    "    fig.legend(legend_lines, [\"Original\", \"Reconstruction\"], loc=\"upper right\")\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "        return fig\n",
    "\n",
    "\n",
    "def visualise_merge_progress(orig, merge_snapshots, show=False):\n",
    "    \"\"\"\n",
    "    Visualize step-by-step merging of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    - orig: original trajectory (N, 2)\n",
    "    - merge_snapshots: list of intermediate merged tokens (arrays of shape M,2)\n",
    "    - show: whether to plt.show() or just return the figure\n",
    "    \"\"\"\n",
    "    if isinstance(orig, torch.Tensor):\n",
    "        orig = orig.detach().cpu().numpy()\n",
    "    merge_snapshots = [s.detach().cpu().numpy() if isinstance(s, torch.Tensor) else s for s in merge_snapshots]\n",
    "\n",
    "    n_steps = len(merge_snapshots)\n",
    "    fig, axes = plt.subplots(1, n_steps + 1, figsize=(5 * (n_steps + 1), 5))\n",
    "\n",
    "    if n_steps == 0:\n",
    "        axes = [axes]\n",
    "    if n_steps == 1:\n",
    "        axes = [axes[0], axes[1]]\n",
    "\n",
    "    # Plot original on first subplot\n",
    "    ax = axes[0]\n",
    "    ax.scatter(orig[:,0], orig[:,1], color='blue', label='Original', s=30)\n",
    "    ax.set_title(\"Original\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Plot merged snapshots step by step\n",
    "    for i, merged in enumerate(merge_snapshots):\n",
    "        ax = axes[i+1]\n",
    "        ax.scatter(orig[:,0], orig[:,1], color='blue', alpha=0.2, label='Original', s=20)\n",
    "        ax.scatter(merged[:,0], merged[:,1], color='red', label=f'Merged step {i+1}', s=50)\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.set_title(f\"Merge step {i+1}\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "        return fig\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device, policy, current_depth, criterion, temperature=1):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "    for batch in val_loader:\n",
    "        raw_tokens = batch.to(device)\n",
    "        buf = TokenMergeBuffer(raw_tokens)\n",
    "\n",
    "        while buf.can_merge(current_depth):\n",
    "            active = buf.get_active_tokens()\n",
    "            B, N, D = active.shape\n",
    "            pair_idx = torch.combinations(torch.arange(N, device=device))\n",
    "            num_pairs = pair_idx.shape[0]\n",
    "\n",
    "            t1 = active[:, pair_idx[:, 0], :]\n",
    "            t2 = active[:, pair_idx[:, 1], :]\n",
    "\n",
    "            t1 = t1.detach()\n",
    "            t2 = t2.detach()\n",
    "             \n",
    "            t1_lifted = model.lift(t1)\n",
    "            t2_lifted = model.lift(t2)\n",
    "\n",
    "            x = torch.cat([t1_lifted, t2_lifted], dim=-1)\n",
    "\n",
    "            x_hat = model(x)\n",
    "\n",
    "            t1_hat_lifted, t2_hat_lifted = torch.chunk(x_hat, 2, dim=-1)                    \n",
    "            t1_hat = model.unlift(t1_hat_lifted) \n",
    "            t2_hat = model.unlift(t2_hat_lifted)  \n",
    "\n",
    "            mse = nn.MSELoss(reduction=\"none\")\n",
    "            recon_t1 = mse(t1, t1_hat).mean(dim=-1)\n",
    "            recon_t2 = mse(t2, t2_hat).mean(dim=-1)\n",
    "            loss_per_pair = recon_t1 + recon_t2\n",
    "\n",
    "            if policy == \"argmin\":\n",
    "                chosen = loss_per_pair.argmin(dim=1)\n",
    "            elif policy == \"softmax\":\n",
    "                probs = torch.softmax(-loss_per_pair / temperature, dim=1)\n",
    "                chosen = torch.multinomial(probs, 1).squeeze(1)\n",
    "            elif policy == \"uniform\":\n",
    "                chosen = torch.randint(0, num_pairs, (B,), device=device)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown policy {policy}\")\n",
    "\n",
    "            local_t1_idx = pair_idx[chosen, 0]\n",
    "            local_t2_idx = pair_idx[chosen, 1]\n",
    "\n",
    "            chosen_t1 = active[torch.arange(B), local_t1_idx, :]\n",
    "            chosen_t2 = active[torch.arange(B), local_t2_idx, :]\n",
    "            chosen_t1 = model.lift(chosen_t1)\n",
    "            chosen_t2 = model.lift(chosen_t2)\n",
    "\n",
    "            chosen_x = torch.cat([chosen_t1, chosen_t2], dim=-1)\n",
    "\n",
    "            merged_tokens = model.encode(chosen_x)\n",
    "            merged_tokens = model.unlift(merged_tokens)\n",
    "\n",
    "            buf.merge_batch(local_t1_idx, local_t2_idx, merged_tokens)\n",
    "\n",
    "        buffer = buf.buffer\n",
    "        merge_history = buf.get_merge_history()\n",
    "        active_mask = buf.get_active_mask()\n",
    "        n_original_tokens = raw_tokens.shape[1]    \n",
    "        unmerge_buf = TokenUnmergeBuffer(buffer=buffer, active_mask=active_mask, merges=merge_history, n_original=n_original_tokens) \n",
    "\n",
    "        while not unmerge_buf.is_done():\n",
    "            merged_token = unmerge_buf.get_next_to_unmerge()\n",
    "            merged_token = model.lift(merged_token)\n",
    "            pred = model.decode(merged_token)\n",
    "            t1_pred, t2_pred = torch.chunk(pred, 2, dim=-1)\n",
    "            t1_pred = model.unlift(t1_pred)\n",
    "            t2_pred = model.unlift(t2_pred)\n",
    "            unmerge_buf.step_unmerge(t1_pred, t2_pred)\n",
    "        \n",
    "        reconstructed = unmerge_buf.get_original_tokens()\n",
    "\n",
    "        loss = criterion(reconstructed, raw_tokens)\n",
    "\n",
    "        total_loss += loss.item() # TODO: eventuell ohne .item()\n",
    "\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        config,\n",
    "        device,\n",
    "        checkpoint_path,\n",
    "        max_depth,\n",
    "        ):\n",
    "    \n",
    "    model.to(device)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #     optimiser, mode='min', factor=0.8, patience=15, verbose=True, min_lr=1e-6\n",
    "    # )    \n",
    "    stopper = EarlyStopper(patience=10, min_delta=1e-4)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"random-walker\",\n",
    "        config=config,\n",
    "        name=config.model_name\n",
    "    )\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        orig_samples = None\n",
    "        recon_samples = None\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimiser.zero_grad()\n",
    "            raw_tokens = batch[0].to(device) if isinstance(batch, (list, tuple)) else batch.to(device)\n",
    "\n",
    "            if orig_samples is None:\n",
    "                orig_samples = raw_tokens\n",
    "\n",
    "            buf = TokenMergeBuffer(raw_tokens)\n",
    "\n",
    "            # current_depth = get_merge_depth(epoch+1, config.epochs, config.max_depth)\n",
    "            current_depth = max_depth\n",
    "            \n",
    "            merge_snapshots_per_sample = [[] for _ in range(5)] \n",
    "\n",
    "            while buf.can_merge(current_depth):\n",
    "                active = buf.get_active_tokens()  # (B, N, D)\n",
    "                B, N, D = active.shape\n",
    "\n",
    "                for i in range(5):\n",
    "                    merge_snapshots_per_sample[i].append(active[i, :, :2].detach().cpu().numpy())\n",
    "\n",
    "                pair_idx = torch.combinations(torch.arange(N, device=device))\n",
    "                num_pairs = pair_idx.shape[0]\n",
    "\n",
    "                t1 = active[:, pair_idx[:, 0], :]\n",
    "                t2 = active[:, pair_idx[:, 1], :]\n",
    "\n",
    "                t1 = t1.detach()\n",
    "                t2 = t2.detach()\n",
    "\n",
    "                t1_lifted = model.lift(t1)\n",
    "                t2_lifted = model.lift(t2)\n",
    "\n",
    "                x = torch.cat([t1_lifted, t2_lifted], dim=-1)\n",
    "\n",
    "                x_hat = model(x)\n",
    "\n",
    "                t1_hat_lifted, t2_hat_lifted = torch.chunk(x_hat, 2, dim=-1)                    \n",
    "                t1_hat = model.unlift(t1_hat_lifted) \n",
    "                t2_hat = model.unlift(t2_hat_lifted)  \n",
    "\n",
    "                mse = nn.MSELoss(reduction=\"none\")\n",
    "                recon_t1 = mse(t1, t1_hat).mean(dim=-1)\n",
    "                recon_t2 = mse(t2, t2_hat).mean(dim=-1)\n",
    "                loss_per_pair = recon_t1 + recon_t2\n",
    "\n",
    "                policy = config.sampling_policy\n",
    "                if policy == \"argmin\":\n",
    "                    chosen = loss_per_pair.argmin(dim=1)\n",
    "                elif policy == \"uniform\":\n",
    "                    chosen = torch.randint(0, num_pairs, (B,), device=device)\n",
    "                elif policy == \"softmax\":\n",
    "                    probs = torch.softmax(-loss_per_pair / config.temperature, dim=1)\n",
    "                    chosen = torch.multinomial(probs, 1).squeeze(1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown sampling policy: {policy}\")\n",
    "                \n",
    "                local_t1_idx = pair_idx[chosen, 0]\n",
    "                local_t2_idx = pair_idx[chosen, 1]\n",
    "\n",
    "                chosen_t1 = active[torch.arange(B), local_t1_idx, :]\n",
    "                chosen_t2 = active[torch.arange(B), local_t2_idx, :]\n",
    "                chosen_t1 = model.lift(chosen_t1)\n",
    "                chosen_t2 = model.lift(chosen_t2)\n",
    "                 \n",
    "                chosen_x = torch.cat([chosen_t1, chosen_t2], dim=-1)\n",
    "\n",
    "                merged_tokens = model.encode(chosen_x)\n",
    "                merged_tokens = model.unlift(merged_tokens)\n",
    "                buf.merge_batch(local_t1_idx, local_t2_idx, merged_tokens)\n",
    "\n",
    "            buffer = buf.buffer\n",
    "            merge_history = buf.get_merge_history()\n",
    "            active_mask = buf.get_active_mask()\n",
    "            n_original_tokens = raw_tokens.shape[1]\n",
    "\n",
    "            unmerge_buf = TokenUnmergeBuffer(buffer=buffer,\n",
    "                                             active_mask=active_mask,\n",
    "                                             merges=merge_history,\n",
    "                                             n_original=n_original_tokens)\n",
    "\n",
    "            while not unmerge_buf.is_done():\n",
    "                merged_token = unmerge_buf.get_next_to_unmerge()\n",
    "                merged_token = model.lift(merged_token)\n",
    "                pred = model.decode(merged_token)\n",
    "                t1_pred, t2_pred = torch.chunk(pred, 2, dim=-1)\n",
    "                t1_pred = model.unlift(t1_pred)\n",
    "                t2_pred = model.unlift(t2_pred)\n",
    "                unmerge_buf.step_unmerge(t1_pred, t2_pred)\n",
    "            reconstructed = unmerge_buf.get_original_tokens()\n",
    "            \n",
    "            if recon_samples is None:\n",
    "                recon_samples = reconstructed\n",
    "\n",
    "            loss = criterion(reconstructed, raw_tokens)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, device, config.sampling_policy, current_depth, criterion, config.temperature)\n",
    "        \n",
    "        # scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "                \n",
    "        if checkpoint_path and val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "            # fig = visualise(orig_samples, recon_samples) \n",
    "            \n",
    "            orig_samples = [orig_samples[i] for i in range(5)]\n",
    "            recon_samples = [recon_samples[i] for i in range(5)]\n",
    "            fig = visualise_multiple(orig_samples, recon_samples)\n",
    "\n",
    "            process_fig = visualise_merge_progress(orig_samples[0], merge_snapshots_per_sample[0])\n",
    "            \n",
    "\n",
    "\n",
    "        log_dict = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"epoch\": epoch,\n",
    "        \"merge_depth\": current_depth,\n",
    "        }\n",
    "        \n",
    "        if 'fig' in locals() and fig is not None:\n",
    "            log_dict[\"reconstruction_plot\"] = wandb.Image(fig)\n",
    "        \n",
    "        if 'scheduler' in locals():\n",
    "            log_dict[\"lr\"] = scheduler.get_last_lr()[0]\n",
    "\n",
    "        # TODO: is broken\n",
    "        # if 'process_fig' in locals() and process_fig is not None:\n",
    "        #     log_dict[\"merge_progress_plot\"] = wandb.Image(process_fig)\n",
    "\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "        # TODO: did stupid shit, is on timeout  \n",
    "        # if stopper.should_stop(val_loss):\n",
    "        #     print(f\"Early stopping at epoch {epoch}\")\n",
    "        #     break\n",
    "\n",
    "        \n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fcb66",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea9188c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/export/home/2jhahn/htm/wandb/run-20251127_042444-3eq5xapf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabianhahn/random-walker/runs/3eq5xapf' target=\"_blank\">AE-softmax-lr-0.0001-latent-4-merges-1</a></strong> to <a href='https://wandb.ai/fabianhahn/random-walker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabianhahn/random-walker' target=\"_blank\">https://wandb.ai/fabianhahn/random-walker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabianhahn/random-walker/runs/3eq5xapf' target=\"_blank\">https://wandb.ai/fabianhahn/random-walker/runs/3eq5xapf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>merge_depth</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>██▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>███▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>499</td></tr><tr><td>merge_depth</td><td>1</td></tr><tr><td>train_loss</td><td>0.00166</td></tr><tr><td>val_loss</td><td>0.00156</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AE-softmax-lr-0.0001-latent-4-merges-1</strong> at: <a href='https://wandb.ai/fabianhahn/random-walker/runs/3eq5xapf' target=\"_blank\">https://wandb.ai/fabianhahn/random-walker/runs/3eq5xapf</a><br> View project at: <a href='https://wandb.ai/fabianhahn/random-walker' target=\"_blank\">https://wandb.ai/fabianhahn/random-walker</a><br>Synced 5 W&B file(s), 500 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251127_042444-3eq5xapf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_dim = 2\n",
    "hidden_dim = 64\n",
    "merges = 1\n",
    "\n",
    "latent_dim = token_dim * (merges+1)  # - epsilon\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    epochs=500,\n",
    "    learning_rate=1e-4,\n",
    "    sampling_policy=\"softmax\",\n",
    "    batch_size=128,\n",
    "    device=device,\n",
    "    model_name=f\"AE-softmax-lr-{1e-4}-latent-{latent_dim}-merges-{merges}\",\n",
    "    max_depth=merges,\n",
    "    latent_dim=latent_dim,\n",
    "    token_dim=token_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "model = AE(\n",
    "    token_dim=token_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=hidden_dim\n",
    "    ).to(device)\n",
    "\n",
    "train(model, train_loader, val_loader, config, device, checkpoint_path=f\"./checkpoints/{config.model_name}.pth\", max_depth=merges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIR2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
